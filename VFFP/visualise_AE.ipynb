{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import Tensor\n",
    "\n",
    "import tifffile\n",
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from customdatasets import SegmentationData\n",
    "from model import VPTREnc, VPTRDec, VPTRDisc, init_weights\n",
    "from model import GDL, MSELoss, L1Loss, GANLoss\n",
    "from utils import get_data\n",
    "from utils import VidCenterCrop, VidPad, VidResize, VidNormalize, VidReNormalize, VidCrop, VidRandomHorizontalFlip, VidRandomVerticalFlip, VidToTensor\n",
    "from utils import visualize_batch_clips, save_ckpt, load_ckpt, set_seed, AverageMeters, init_loss_dict, update_summary, write_summary, resume_training\n",
    "from utils import set_seed\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "from model import VPTREnc, VPTRDec, VPTRDisc, init_weights, VPTRFormerNAR, VPTRFormerFAR\n",
    "from model import GDL, MSELoss, L1Loss, GANLoss, BiPatchNCE\n",
    "from utils import KTHDataset, BAIRDataset, MovingMNISTDataset\n",
    "from utils import get_dataloader, get_data\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(2021)\n",
    "ckpt_save_dir = Path(working_dir+'trained_transformer')\n",
    "tensorboard_save_dir = Path(working_dir+'tensorboard/')\n",
    "resume_AE_ckpt = Path(working_dir+'trained_ae').joinpath('epoch_25.tar')\n",
    "#resume_ckpt = ckpt_save_dir.joinpath('epoch_179.tar')\n",
    "resume_ckpt = None\n",
    "\n",
    "#############Set the logger#########\n",
    "if not Path(ckpt_save_dir).exists():\n",
    "        Path(ckpt_save_dir).mkdir(parents=True, exist_ok=True)\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                datefmt='%a, %d %b %Y %H:%M:%S',\n",
    "                format='%(asctime)s - %(message)s',\n",
    "                filename=ckpt_save_dir.joinpath('train_log.log').absolute().as_posix(),\n",
    "                filemode='a')\n",
    "\n",
    "start_epoch = 0\n",
    "summary_writer = SummaryWriter(tensorboard_save_dir.absolute().as_posix())\n",
    "num_past_frames = 15\n",
    "num_future_frames = 5\n",
    "encH, encW, encC = 16, 16, 528\n",
    "img_channels = 1 #3 channels for BAIR\n",
    "epochs = 3\n",
    "N = 1\n",
    "#AE_lr = 2e-4\n",
    "Transformer_lr = 1e-4\n",
    "max_grad_norm = 1.0 \n",
    "rpe = False\n",
    "lam_gan = 0.001\n",
    "dropout = 0.1\n",
    "device = torch.device('cuda:0')\n",
    "val_per_epochs = 4\n",
    "\n",
    "#####################Init Dataset ###########################\n",
    "data_set_name = 'CSD'\n",
    "dataset_dir = working_dir+'MCS/normalized'\n",
    "test_past_frames = 10\n",
    "test_future_frames = 10\n",
    "# train_loader, val_loader, test_loader, renorm_transform = get_data(N, dataset_dir)#, num_frames = 100, video_range = 1000)\n",
    "train_loader, val_loader, test_loader, renorm_transform = get_data(N, dataset_dir, num_frames = 20, video_limit = None)\n",
    "\n",
    "#####################Init model###########################\n",
    "VPTR_Enc = VPTREnc(img_channels, feat_dim = encC, n_downsampling = 3).to(device)\n",
    "VPTR_Dec = VPTRDec(img_channels, feat_dim = encC, n_downsampling = 3, out_layer = 'Sigmoid').to(device) #Tanh for KTH and BAIR\n",
    "VPTR_Enc = VPTR_Enc.eval()\n",
    "VPTR_Dec = VPTR_Dec.eval()\n",
    "\n",
    "VPTR_Disc = None\n",
    "#VPTR_Disc = VPTRDisc(img_channels, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d).to(device)\n",
    "#VPTR_Disc = VPTR_Disc.eval()\n",
    "#init_weights(VPTR_Disc)\n",
    "init_weights(VPTR_Enc)\n",
    "init_weights(VPTR_Dec)\n",
    "\n",
    "VPTR_Transformer = VPTRFormerFAR(num_past_frames, num_future_frames, encH=encH, encW = encW, d_model=encC, \n",
    "                            nhead=8, num_encoder_layers=12, dropout=dropout, \n",
    "                            window_size=4, Spatial_FFN_hidden_ratio=4, rpe=rpe).to(device)\n",
    "\n",
    "optimizer_D = None\n",
    "#optimizer_D = torch.optim.Adam(params = VPTR_Disc.parameters(), lr = Transformer_lr, betas = (0.5, 0.999))\n",
    "optimizer_T = torch.optim.AdamW(params = VPTR_Transformer.parameters(), lr = Transformer_lr)\n",
    "\n",
    "Transformer_parameters = sum(p.numel() for p in VPTR_Transformer.parameters() if p.requires_grad)\n",
    "print(f\"FAR Transformer num_parameters: {Transformer_parameters}\")\n",
    "\n",
    "#####################Init loss function###########################\n",
    "loss_name_list = ['T_MSE', 'T_GDL', 'T_gan', 'T_total', 'Dtotal', 'Dfake', 'Dreal']\n",
    "#gan_loss = GANLoss('vanilla', target_real_label=1.0, target_fake_label=0.0).to(device)\n",
    "loss_dict = init_loss_dict(loss_name_list)\n",
    "mse_loss = MSELoss()\n",
    "gdl_loss = GDL(alpha = 1)\n",
    "\n",
    "#load the trained autoencoder, we initialize the discriminator from scratch, for a balanced training\n",
    "loss_dict, start_epoch = resume_training({'VPTR_Enc': VPTR_Enc, 'VPTR_Dec': VPTR_Dec}, {}, resume_AE_ckpt, loss_name_list)\n",
    "\n",
    "if resume_ckpt is not None:\n",
    "    loss_dict, start_epoch = resume_training({'VPTR_Transformer': VPTR_Transformer}, \n",
    "                                            {'optimizer_T':optimizer_T}, resume_ckpt, loss_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = tifffile.memmap(working_dir + 'MCS/normalized/200803/01_baseline_norm.tif', mode='r+')\n",
    "x = vid[:100]\n",
    "\n",
    "# # flatten the array into a 1D array\n",
    "# flat_arr = x.flatten()\n",
    "\n",
    "# # calculate the mean and standard deviation of the flattened array\n",
    "# mean = np.mean(flat_arr)\n",
    "# std = np.std(flat_arr)\n",
    "\n",
    "# print(\"Mean:\", mean)\n",
    "# print(\"Standard deviation:\", std)\n",
    "x = torch.from_numpy(x).unsqueeze(1).unsqueeze(0)\n",
    "x = F.interpolate(x, size=(1, 128, 128), mode='nearest')\n",
    "# for i in range(T):\n",
    "#   x[i, ...] = transforms.Normalize(1.0937392, 0.11474588)(x[i, ...])\n",
    "x = transforms.Normalize(1.0937392, 0.11474588)(x)\n",
    "plt.imshow(x[0, 0, 0])  # Show the first frame of the normalized tensor\n",
    "\n",
    "x = x.to(device)\n",
    "print(x.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "# print(rec_frames.shape)\n",
    "x_frame = x[0][50].squeeze().cpu()\n",
    "with torch.no_grad():\n",
    "  rec_frames = VPTR_Dec(VPTR_Enc(x))\n",
    "  pred_frame = rec_frames[0][50].squeeze().cpu()\n",
    "  p_r = F.interpolate(rec_frames, size=(1, 256, 256), mode='nearest')\n",
    "\n",
    "  axs[0].imshow(x_frame)\n",
    "  axs[1].imshow(pred_frame)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
